{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Решение задачи классификации текстов по сантименту при помощи нейронных сетей-4\n",
    "\n",
    "### Эпиграф\n",
    "\n",
    "Будучи глубоко неудовлетворен теми результатами, которых позволяла достичь линейная и логистическая регрессия, я решил отложить сдачу финального проекта до тех пор, пока не обучусь нейросетям. Т.к. в настоящей специализации они проходились, я считаю такое решение задачи нейросетями совершенно легитимным.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rookie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rookie\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers as L\n",
    "import tensorflow.compat.v1 as v1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000, 3903)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.read_csv(\"_data\\products_sentiment_train.tsv\", sep='\\t', header=None)\n",
    "dft.columns = [\"text\", \"label\"]\n",
    "\n",
    "dfv = pd.read_csv(\"_data\\products_sentiment_test.tsv\", sep='\\t')\n",
    "\n",
    "X = [x[0] for x in dft[[\"text\"]].values.tolist()]\n",
    "Y = [y[0] for y in dft[[\"label\"]].values.tolist()]\n",
    "X_test = [x[0] for x in dfv[[\"text\"]].values.tolist()]\n",
    "\n",
    "def tokenize(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in text.lower().split() if w not in stopwords and w.isalnum()]\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    return [tokenize(text) for text in texts]\n",
    "\n",
    "Xt = tokenize_texts(X)\n",
    "Xtest_t = tokenize_texts(X_test)\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "START = \"<START>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "all_words = [PAD, START, END, UNK] + list(set([item for sublist in Xt + Xtest_t for item in sublist]))\n",
    "vocab = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "len(Xt), len(Y), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(r\"_data\\vocab.pckl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['2', 'take', 'around', '640x480', 'picture'],\n",
       "  ['downloaded',\n",
       "   'trial',\n",
       "   'version',\n",
       "   'computer',\n",
       "   'associate',\n",
       "   'ez',\n",
       "   'firewall',\n",
       "   'antivirus',\n",
       "   'fell',\n",
       "   'love',\n",
       "   'computer',\n",
       "   'security',\n",
       "   'system'],\n",
       "  ['wrt54g',\n",
       "   'plus',\n",
       "   'hga7t',\n",
       "   'perfect',\n",
       "   'solution',\n",
       "   'need',\n",
       "   'wireless',\n",
       "   'coverage',\n",
       "   'wider',\n",
       "   'area',\n",
       "   'house',\n",
       "   'case']],\n",
       " array([[   1, 2935,  579,  841,  229, 1192,    2,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0],\n",
       "        [   1,  350, 3448,  906,  712,  989, 2532, 2902, 3428, 3165, 2225,\n",
       "          712, 1430,  954,    2],\n",
       "        [   1,  283, 3187,  422,  869, 3397, 2791,  610,  870, 1295, 3028,\n",
       "         1348, 2394,    2,    0]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.1 Функции для работы модели\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def to_sequence(text):\n",
    "    return np.array([1] + [vocab[w] if w in vocab else vocab[UNK] for w in text] + [2])\n",
    "\n",
    "def to_matrix(texts, maxlen=0):\n",
    "    seqs = [to_sequence(text) for text in texts]\n",
    "    if maxlen == 0:\n",
    "        maxlen = min(9999, max(list(map(len, seqs))))\n",
    "    return pad_sequences(seqs, maxlen=maxlen, dtype='int32', padding='post', truncating='post', value=0)\n",
    "\n",
    "Xt[:3], to_matrix(Xt[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, None, 64)          249792    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 324,161\n",
      "Trainable params: 324,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#5.1. Модель 1 - дает точность 0.7666\n",
    "N_LSTM = 64\n",
    "\n",
    "def build_model():\n",
    "    X = L.Input(batch_input_shape=(None, None))\n",
    "    e = L.Embedding(len(vocab), 64, mask_zero=True)(X)\n",
    "    l1 = L.Bidirectional(L.LSTM(units=N_LSTM, return_sequences=False, dropout=0.25))(e)    \n",
    "    d1 = L.Dense(64, activation='relu')(l1)\n",
    "    d1 = L.Dropout(0.5)(d1) \n",
    "    Y = L.Dense(1, activation='sigmoid')(d1)\n",
    "    return keras.models.Model(inputs=X, outputs=Y)\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1400/1400 [==============================] - 20s 14ms/step - loss: 0.5819 - binary_accuracy: 0.6843 - val_loss: 0.5086 - val_binary_accuracy: 0.7633\n",
      "Epoch 2/5\n",
      "1400/1400 [==============================] - 17s 12ms/step - loss: 0.3038 - binary_accuracy: 0.8829 - val_loss: 0.5687 - val_binary_accuracy: 0.7450\n",
      "Epoch 3/5\n",
      "1400/1400 [==============================] - 17s 12ms/step - loss: 0.1353 - binary_accuracy: 0.9557 - val_loss: 0.8404 - val_binary_accuracy: 0.7467\n",
      "Epoch 4/5\n",
      "1400/1400 [==============================] - 18s 13ms/step - loss: 0.0708 - binary_accuracy: 0.9807 - val_loss: 0.9896 - val_binary_accuracy: 0.7250\n",
      "Epoch 5/5\n",
      "1400/1400 [==============================] - 17s 12ms/step - loss: 0.0402 - binary_accuracy: 0.9893 - val_loss: 1.2627 - val_binary_accuracy: 0.7083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22b2a6a3ca0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t, X_v, Y_t, Y_v = train_test_split(Xt, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss=\"binary_crossentropy\", metrics=['binary_accuracy'])\n",
    "model.fit(to_matrix(X_t), np.array(Y_t), validation_data=(to_matrix(X_v), np.array(Y_v)), initial_epoch=0, epochs=5, batch_size=1,\n",
    "         callbacks=[tf.keras.callbacks.ModelCheckpoint('./best_model2.hdf5', monitor='val_binary_accuracy', verbose=0, save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['small',\n",
       "  'digital',\n",
       "  'elph',\n",
       "  'rather',\n",
       "  'one',\n",
       "  'camera',\n",
       "  'better',\n",
       "  'resolution',\n",
       "  'picture',\n",
       "  'quality',\n",
       "  'size',\n",
       "  '2',\n",
       "  'unless',\n",
       "  'small',\n",
       "  'cary',\n",
       "  'around'],\n",
       " ['way',\n",
       "  'first',\n",
       "  'disk',\n",
       "  'played',\n",
       "  'naturally',\n",
       "  '31',\n",
       "  'day',\n",
       "  'purchase',\n",
       "  'dvd',\n",
       "  'player',\n",
       "  'froze'],\n",
       " ['better', 'zen', 'micro', 'outlook', 'compatibility']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest_t[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = [0 if y < 0.5 else 1 for y in model.predict(to_matrix(Xtest_t))]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"y\"] = Y_pred\n",
    "df.to_csv(\"kaggle_submission.csv\", sep=',', index_label=\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(filename = 'screen.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
