{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of practice_reinforce.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPDR_U-BE5O_"
      },
      "source": [
        "# REINFORCE in TensorFlow\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a TensorFlow network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1YUEwx2E5PE"
      },
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules:\n",
        "    %tensorflow_version 1.x\n",
        "    \n",
        "    if not os.path.exists('.setup_complete'):\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "        !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "        !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geeR60XnE5PE"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0ms2OKDE5PF"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1efuSUKE5PF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "d5f4cd3d-bc56-40b3-a344-e6c428a040fb"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))\n",
        "env.close()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjtIVh3YE5PF"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jp1Bn__YE5PF"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDJw09mIE5PG"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuGPg-kuE5PG"
      },
      "source": [
        "# create input variables. We only need <s, a, r> for REINFORCE\n",
        "states = tf.placeholder('float32',(None,)+state_dim,name=\"states\")\n",
        "actions = tf.placeholder('int32',name=\"action_ids\")\n",
        "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x99T9vlE5PG"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import keras.layers as L\n",
        "\n",
        "logits = L.Dense(n_actions)(states)\n",
        "\n",
        "policy = tf.nn.softmax(logits)\n",
        "log_policy = tf.nn.log_softmax(logits)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdi44IhVE5PH"
      },
      "source": [
        "#utility function to pick action in one given state\n",
        "get_action_proba = lambda s: policy.eval({states:[s]})[0]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkLp_o3eE5PH"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN8ekB2vE5PH"
      },
      "source": [
        "def get_cumulative_rewards(rewards, #rewards at each step\n",
        "                           gamma = 0.99 #discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    take a list of immediate rewards r(s,a) for the whole session \n",
        "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
        "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "    \n",
        "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
        "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
        "    \n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    cum_rewards = [0] * len(rewards)\n",
        "    cum_rewards[-1] = rewards[-1]\n",
        "    for i in reversed(range(len(rewards)-1)):\n",
        "        cum_rewards[i] = rewards[i] + gamma * cum_rewards[i+1]\n",
        "        \n",
        "    return cum_rewards"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXWerAFAE5PI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fe9bd37-84b1-4121-b051-6fb964e7e132"
      },
      "source": [
        "assert len(get_cumulative_rewards(range(100))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzMFpeowE5PI"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse Tensorflow's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn3SRjAAE5PI"
      },
      "source": [
        "#get probabilities for parti\n",
        "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
        "log_policy_for_actions = tf.gather_nd(log_policy, indices)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziqKMqaxE5PJ"
      },
      "source": [
        "# Policy objective as in the last formula. Please use reduce_mean, not reduce_sum.\n",
        "# You may use log_policy_for_actions to get log probabilities for actions taken.\n",
        "# Also recall that we defined ph_cumulative_rewards earlier.\n",
        "\n",
        "J = tf.reduce_mean(log_policy_for_actions*cumulative_rewards)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1j8jD9gPE5PJ"
      },
      "source": [
        "As a reminder, for a discrete probability distribution (like the one our policy outputs), entropy is defined as:\n",
        "\n",
        "$$ \\operatorname{entropy}(p) = -\\sum_{i = 1}^n p_i \\cdot \\log p_i $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGNu0z_bE5PJ"
      },
      "source": [
        "# Entropy regularization. If you don't add it, the policy will quickly deteriorate to\n",
        "# being deterministic, harming exploration.\n",
        "\n",
        "entropy = -tf.reduce_sum(policy * log_policy)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvG_r8rOE5PK"
      },
      "source": [
        "# # Maximizing X is the same as minimizing -X, hence the sign.\n",
        "loss = -(J + 0.001 * entropy)\n",
        "\n",
        "update = tf.train.AdamOptimizer().minimize(loss)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaqzrPCOE5PK"
      },
      "source": [
        "def train_on_session(states, actions, rewards, t_max=1000):\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
        "    cumulative_rewards = get_cumulative_rewards(rewards)\n",
        "    update.run({\n",
        "        states: states,\n",
        "        actions: actions,\n",
        "        cumulative_rewards: cumulative_rewards,\n",
        "    })\n",
        "    return sum(rewards)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSAX1wePXJn5"
      },
      "source": [
        "def train_step(_states,_actions,_rewards):\r\n",
        "    \"\"\"given full session, trains agent with policy gradient\"\"\"\r\n",
        "    _cumulative_rewards = get_cumulative_rewards(_rewards)\r\n",
        "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-_dFsRiE5PK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f865f7f-ffe3-47c1-8b8b-5f52108bb343"
      },
      "source": [
        "# Initialize optimizer parameters\n",
        "s = tf.InteractiveSession()\n",
        "s.run(tf.global_variables_initializer())"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvq9YFnaE5PH"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbuZ-UJsE5PH"
      },
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
        "    \n",
        "    #arrays to record session\n",
        "    states,actions,rewards = [],[],[]    \n",
        "    s = env.reset()\n",
        "    \n",
        "    for t in range(t_max):\n",
        "        \n",
        "        #action probabilities array aka pi(a|s)\n",
        "        action_probas = get_action_proba(s)        \n",
        "        a = np.random.choice([0,1],p=action_probas)        \n",
        "        new_s,r,done,info = env.step(a)\n",
        "        \n",
        "        #record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "        \n",
        "        s = new_s\n",
        "        if done: break\n",
        "            \n",
        "    train_step(states,actions,rewards)\n",
        "            \n",
        "    return sum(rewards)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7uPscEHE5PK"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "rCtKZXtFH7lY",
        "outputId": "915e1ba4-8131-4f9f-8e6f-e0dab6793836"
      },
      "source": [
        "env = gym.make(\"CartPole-v0\")\r\n",
        "\r\n",
        "#gym compatibility: unwrap TimeLimit\r\n",
        "if hasattr(env,'env'):\r\n",
        "    env=env.env\r\n",
        "\r\n",
        "env.reset()\r\n",
        "n_actions = env.action_space.n\r\n",
        "state_dim = env.observation_space.shape\r\n",
        "\r\n",
        "plt.imshow(env.render(\"rgb_array\"))\r\n",
        "\r\n",
        "env.close()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATM0lEQVR4nO3df6xc5Z3f8ffHPzAmP9Y43HVc26zZxBVLVo2JbghRshJLlF1Cq8JKaQRtgY2QvJWIlEhRW9hK3UQq0a7SDWnUDa1XEEiThtBNCBYiTbxAtcpKAUziEINhuUlMsWtj8xsDMbH97R/3mAxwzZ37y+Pnzvsljeac73nOzPcR4w9zn3vmTqoKSVI7Fgy6AUnS1BjcktQYg1uSGmNwS1JjDG5JaozBLUmNmbPgTnJekoeTjCW5cq6eR5KGTebiOu4kC4F/AD4M7ATuBS6uqgdn/ckkacjM1Tvus4Cxqvp5Vb0M3ARcMEfPJUlDZdEcPe4q4LGe/Z3A+442+JRTTqm1a9fOUSuS1J4dO3bwxBNPZKJjcxXck0qyAdgAcOqpp7Jly5ZBtSJJx53R0dGjHpurpZJdwJqe/dVd7RVVtbGqRqtqdGRkZI7akKT5Z66C+15gXZLTkpwAXARsmqPnkqShMidLJVV1MMkngO8BC4Hrq+qBuXguSRo2c7bGXVW3A7fP1eNL0rDyk5OS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhozo68uS7IDeB44BBysqtEky4FvAmuBHcDHqurpmbUpSTpiNt5x/35Vra+q0W7/SuCOqloH3NHtS5JmyVwslVwA3Nht3whcOAfPIUlDa6bBXcD3k9yXZENXW1FVu7vtPcCKGT6HJKnHjNa4gQ9W1a4kvwlsTvJQ78GqqiQ10Yld0G8AOPXUU2fYhiQNjxm9466qXd39XuAW4Czg8SQrAbr7vUc5d2NVjVbV6MjIyEzakKShMu3gTvKmJG85sg38AbAN2ARc1g27DLh1pk1Kkn5tJkslK4Bbkhx5nP9ZVf87yb3AzUkuBx4FPjbzNiVJR0w7uKvq58C7J6g/CXxoJk1Jko7OT05KUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjZk0uJNcn2Rvkm09teVJNid5pLs/uasnyZeSjCW5P8l75rJ5SRpG/bzjvgE47zW1K4E7qmodcEe3D/ARYF132wBcOzttSpKOmDS4q+rvgKdeU74AuLHbvhG4sKf+1Rr3Q2BZkpWz1awkafpr3Cuqane3vQdY0W2vAh7rGbezq71Okg1JtiTZsm/fvmm2IUnDZ8a/nKyqAmoa522sqtGqGh0ZGZlpG5I0NKYb3I8fWQLp7vd29V3Amp5xq7uaJGmWTDe4NwGXdduXAbf21C/tri45G3i2Z0lFkjQLFk02IMk3gHOAU5LsBP4M+HPg5iSXA48CH+uG3w6cD4wBLwIfn4OeJWmoTRrcVXXxUQ59aIKxBVwx06YkSUfnJyclqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDVm0uBOcn2SvUm29dQ+k2RXkq3d7fyeY1clGUvycJI/nKvGJWlY9fOO+wbgvAnq11TV+u52O0CSM4CLgHd153w5ycLZalaS1EdwV9XfAU/1+XgXADdV1YGq+gXj3/Z+1gz6kyS9xkzWuD+R5P5uKeXkrrYKeKxnzM6u9jpJNiTZkmTLvn37ZtCGJA2X6Qb3tcA7gPXAbuAvp/oAVbWxqkaranRkZGSabUjS8JlWcFfV41V1qKoOA3/Nr5dDdgFreoau7mqSpFkyreBOsrJn94+AI1ecbAIuSrIkyWnAOuCembUoSeq1aLIBSb4BnAOckmQn8GfAOUnWAwXsAP4EoKoeSHIz8CBwELiiqg7NTeuSNJwmDe6quniC8nVvMP5q4OqZNCVJOjo/OSlJjTG4JakxBrckNcbglqTGGNyS1JhJryqR5rsX9j3KoZdf4sRlb+eENy0bdDvSpAxuDZ2DB15kx/+5gTr0KwBe2LuDQy+/yKm/968Z+Z3fG3B30uQMbg2dOnSQ53dt5/DBlwfdijQtrnFLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1uSGmNwS1JjDG5JasykwZ1kTZK7kjyY5IEkn+zqy5NsTvJId39yV0+SLyUZS3J/kvfM9SQkaZj08477IPDpqjoDOBu4IskZwJXAHVW1Drij2wf4COPf7r4O2ABcO+tdS9IQmzS4q2p3Vf2o234e2A6sAi4AbuyG3Qhc2G1fAHy1xv0QWJZk5ax3LklDakpr3EnWAmcCdwMrqmp3d2gPsKLbXgU81nPazq722sfakGRLki379u2bYtvSDATIBC/9OkxVHfN2pKnqO7iTvBn4FvCpqnqu91iNv9qn9Iqvqo1VNVpVoyMjI1M5VZqRRUvezCmnf/B19b3b7nzlb3RLx7O+gjvJYsZD++tV9e2u/PiRJZDufm9X3wWs6Tl9dVeTjgtZsICFJyx9Xf3ggRd9x60m9HNVSYDrgO1V9YWeQ5uAy7rty4Bbe+qXdleXnA0827OkIkmaoX6+AecDwCXAT5Ns7Wp/Cvw5cHOSy4FHgY91x24HzgfGgBeBj89qx5I05CYN7qr6AeO/zpnIhyYYX8AVM+xLknQUfnJSkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY0xuCWpMQa3JDXG4JakxhjcktQYg1s6og5z6OUXB92FNCmDW0PpratOZ8HiE19VO/jL/Tzx0N8PqCOpf/18WfCaJHcleTDJA0k+2dU/k2RXkq3d7fyec65KMpbk4SR/OJcTkKZj6dvWsGDRCRMc8Vvedfzr58uCDwKfrqofJXkLcF+Szd2xa6rqP/cOTnIGcBHwLuAfAX+b5B9X1aHZbFyShtWk77irandV/ajbfh7YDqx6g1MuAG6qqgNV9QvGv+39rNloVpI0xTXuJGuBM4G7u9Inktyf5PokJ3e1VcBjPaft5I2DXpI0BX0Hd5I3A98CPlVVzwHXAu8A1gO7gb+cyhMn2ZBkS5It+/btm8qpkjTU+gruJIsZD+2vV9W3Aarq8ao6VFWHgb/m18shu4A1Paev7mqvUlUbq2q0qkZHRkZmMgdJGir9XFUS4Dpge1V9oae+smfYHwHbuu1NwEVJliQ5DVgH3DN7LUvScOvnqpIPAJcAP02ytav9KXBxkvWMXz+1A/gTgKp6IMnNwIOMX5FyhVeUSNLsmTS4q+oHQCY4dPsbnHM1cPUM+pIkHYWfnJSkxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuDaUFixZzyukfeF39qbF7OfjLFwbQkdS/fv6sq9SMe++9l8997nN9jT1zzYn8099966tqzz+5hz/+40t54cDhSc9fvnw5X/7yl1myZMm0epWmy+DWvPL444/zne98p7/BHzyd8951DgcPHwne4tCh/Xz3u9/lqedemvT0lStXcuiQf2pex57BraFVLOCh597H/33pdAAW5lecvvT7A+5KmpzBraH1/156BztefBfV/arnUC3m0RfP4HD5z0LHN385qaF1qBa+EtpH7Duwhl8dXjygjqT+9PNlwScmuSfJT5I8kOSzXf20JHcnGUvyzSQndPUl3f5Yd3zt3E5Bmp4lC15iAQdfVVu1dIwTFhwYUEdSf/p5x30AOLeq3g2sB85LcjbwF8A1VfVO4Gng8m785cDTXf2abpx03Flx4qP8zlvv5k0Ln+GF53fx9JOPsGD/3+N3W+t418+XBRewv9td3N0KOBf4l139RuAzwLXABd02wN8A/zVJuseRjhtbx/aQW/4bBdyzfRe7n9xPKA77UtVxrq/fwiRZCNwHvBP4K+BnwDNVdeTnzJ3Aqm57FfAYQFUdTPIs8DbgiaM9/p49e/j85z8/rQlIvbZv39732B17nmHHnmdeVZtKZO/fv58vfvGLLF7smrhm3549e456rK/grvGfHdcnWQbcApw+06aSbAA2AKxatYpLLrlkpg8psXnzZr7yla8ck+c66aSTuPjii1m6dOkxeT4Nl6997WtHPTal656q6pkkdwHvB5YlWdS9614N7OqG7QLWADuTLAJ+A3hygsfaCGwEGB0drbe//e1TaUWa0Mknn3zMnmvBggWsWLGCk0466Zg9p4bHG/0k189VJSPdO22SLAU+DGwH7gI+2g27DLi1297U7dMdv9P1bUmaPf28414J3Nitcy8Abq6q25I8CNyU5D8BPwau68ZfB/yPJGPAU8BFc9C3JA2tfq4quR84c4L6z4GzJqj/EvgXs9KdJOl1/OSkJDXG4JakxvjXdDSvrFixggsvvPCYPNfy5ctZuHDhMXkuqZfBrXnlve99L7fccsug25DmlEslktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4Jakx/XxZ8IlJ7knykyQPJPlsV78hyS+SbO1u67t6knwpyViS+5O8Z64nIUnDpJ+/x30AOLeq9idZDPwgyXe7Y/+2qv7mNeM/Aqzrbu8Dru3uJUmzYNJ33DVuf7e7uLvVG5xyAfDV7rwfAsuSrJx5q5Ik6HONO8nCJFuBvcDmqrq7O3R1txxyTZIlXW0V8FjP6Tu7miRpFvQV3FV1qKrWA6uBs5L8LnAVcDrwXmA58O+n8sRJNiTZkmTLvn37pti2JA2vKV1VUlXPAHcB51XV7m455ADwFeCsbtguYE3Paau72msfa2NVjVbV6MjIyPS6l6Qh1M9VJSNJlnXbS4EPAw8dWbdOEuBCYFt3yibg0u7qkrOBZ6tq95x0L0lDqJ+rSlYCNyZZyHjQ31xVtyW5M8kIEGAr8G+68bcD5wNjwIvAx2e/bUkaXpMGd1XdD5w5Qf3co4wv4IqZtyZJmoifnJSkxhjcktQYg1uSGmNwS1JjDG5JaozBLUmNMbglqTEGtyQ1xuCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4JakxBrckNcbglqTGGNyS1BiDW5IaY3BLUmMMbklqjMEtSY1JVQ26B5I8Dzw86D7myCnAE4NuYg7M13nB/J2b82rLb1XVyEQHFh3rTo7i4aoaHXQTcyHJlvk4t/k6L5i/c3Ne84dLJZLUGINbkhpzvAT3xkE3MIfm69zm67xg/s7Nec0Tx8UvJyVJ/Tte3nFLkvo08OBOcl6Sh5OMJbly0P1MVZLrk+xNsq2ntjzJ5iSPdPcnd/Uk+VI31/uTvGdwnb+xJGuS3JXkwSQPJPlkV296bklOTHJPkp908/psVz8tyd1d/99MckJXX9Ltj3XH1w6y/8kkWZjkx0lu6/bny7x2JPlpkq1JtnS1pl+LMzHQ4E6yEPgr4CPAGcDFSc4YZE/TcANw3mtqVwJ3VNU64I5uH8bnua67bQCuPUY9TsdB4NNVdQZwNnBF99+m9bkdAM6tqncD64HzkpwN/AVwTVW9E3gauLwbfznwdFe/pht3PPsksL1nf77MC+D3q2p9z6V/rb8Wp6+qBnYD3g98r2f/KuCqQfY0zXmsBbb17D8MrOy2VzJ+nTrAfwcunmjc8X4DbgU+PJ/mBpwE/Ah4H+Mf4FjU1V95XQLfA97fbS/qxmXQvR9lPqsZD7BzgduAzId5dT3uAE55TW3evBanehv0Uskq4LGe/Z1drXUrqmp3t70HWNFtNznf7sfoM4G7mQdz65YTtgJ7gc3Az4BnqupgN6S391fm1R1/Fnjbse24b18E/h1wuNt/G/NjXgAFfD/JfUk2dLXmX4vTdbx8cnLeqqpK0uylO0neDHwL+FRVPZfklWOtzq2qDgHrkywDbgFOH3BLM5bknwF7q+q+JOcMup858MGq2pXkN4HNSR7qPdjqa3G6Bv2Oexewpmd/dVdr3eNJVgJ093u7elPzTbKY8dD+elV9uyvPi7kBVNUzwF2MLyEsS3LkjUxv76/Mqzv+G8CTx7jVfnwA+OdJdgA3Mb5c8l9of14AVNWu7n4v4/+zPYt59FqcqkEH973Auu433ycAFwGbBtzTbNgEXNZtX8b4+vCR+qXdb73PBp7t+VHvuJLxt9bXAdur6gs9h5qeW5KR7p02SZYyvm6/nfEA/2g37LXzOjLfjwJ3VrdwejypqquqanVVrWX839GdVfWvaHxeAEnelOQtR7aBPwC20fhrcUYGvcgOnA/8A+PrjP9h0P1Mo/9vALuBXzG+lnY542uFdwCPAH8LLO/GhvGraH4G/BQYHXT/bzCvDzK+rng/sLW7nd/63IB/Avy4m9c24D929d8G7gHGgP8FLOnqJ3b7Y93x3x70HPqY4znAbfNlXt0cftLdHjiSE62/Fmdy85OTktSYQS+VSJKmyOCWpMYY3JLUGINbkhpjcEtSYwxuSWqMwS1JjTG4Jakx/x9pOotQphBEewAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CALSizMlE5PK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606f1c4e-7679-4b0d-9a03-40b310a777f0"
      },
      "source": [
        "for i in range(100):    \n",
        "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
        "    \n",
        "    print (\"epoch %d, mean reward:%.3f\"%(i, np.mean(rewards)))\n",
        "\n",
        "    if np.mean(rewards) > 300:\n",
        "        print (\"You Win!\")\n",
        "        break"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0, mean reward:240.040\n",
            "epoch 1, mean reward:286.840\n",
            "epoch 2, mean reward:251.560\n",
            "epoch 3, mean reward:244.530\n",
            "epoch 4, mean reward:210.810\n",
            "epoch 5, mean reward:249.330\n",
            "epoch 6, mean reward:274.490\n",
            "epoch 7, mean reward:299.690\n",
            "epoch 8, mean reward:349.860\n",
            "You Win!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPiDmMEOE5PL"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CYDWyrQE5PL"
      },
      "source": [
        "#record sessions\n",
        "import gym\n",
        "import gym.wrappers\n",
        "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
        "sessions = [generate_session() for _ in range(100)]"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlCyeE-DE5PL"
      },
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpn-Xuy-b0TJ"
      },
      "source": [
        "def generate_session(env, t_max=1000):\r\n",
        "    \"\"\" \r\n",
        "    Play a full session with REINFORCE agent.\r\n",
        "    Returns sequences of states, actions, and rewards.\r\n",
        "    \"\"\"\r\n",
        "    # arrays to record session\r\n",
        "    states, actions, rewards = [], [], []\r\n",
        "    s = env.reset()\r\n",
        "\r\n",
        "    for t in range(t_max):\r\n",
        "        # action probabilities array aka pi(a|s)\r\n",
        "        action_probas = get_action_proba(s)        \r\n",
        "        a = np.random.choice([0,1],p=action_probas)\r\n",
        "        new_s, r, done, info = env.step(a)\r\n",
        "\r\n",
        "        # record session history to train later\r\n",
        "        states.append(s)\r\n",
        "        actions.append(a)\r\n",
        "        rewards.append(r)\r\n",
        "\r\n",
        "        s = new_s\r\n",
        "        if done:\r\n",
        "            break\r\n",
        "\r\n",
        "    return states, actions, rewards"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X05VrJg2E5PL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ebfa70c-93fa-494c-f77b-e5bd279ee016"
      },
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'groddenator@gmail.com', 'LL49sVCoKXS1BShs')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your average reward is 301.56 over 100 episodes\n",
            "Submitted to Coursera platform. See results on assignment page!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzglZq6sE5PL"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ]
}